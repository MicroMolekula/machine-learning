# Анализ и объяснение кода в `lab2.ipynb`

Этот документ содержит подробное описание и объяснение кода, представленного в Jupyter ноутбуке `lab2.ipynb`. Ноутбук посвящен задаче классификации ирисов с использованием различных методов машинного обучения.

## 1. Введение и импорт библиотек

В начале ноутбука импортируются все необходимые библиотеки для анализа данных, машинного обучения и визуализации.

```python
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from ucimlrepo import fetch_ucirepo
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.inspection import permutation_importance
```

- **pandas**: для работы с данными в виде таблиц (DataFrame).
- **sklearn**: основная библиотека для машинного обучения в Python.
    - `GaussianNB`: реализация наивного байесовского классификатора.
    - `DecisionTreeClassifier`: реализация дерева решений.
    - `SVC`: реализация метода опорных векторов (Support Vector Classifier).
    - `metrics`: для оценки качества моделей (матрица ошибок, точность и т.д.).
    - `model_selection`: для разделения данных на обучающую и тестовую выборки.
    - `preprocessing`: для предварительной обработки данных (кодирование меток, масштабирование).
    - `decomposition`: для понижения размерности (используется PCA).
    - `inspection`: для анализа важности признаков.
- **matplotlib.pyplot** и **seaborn**: для визуализации данных и результатов (графики, матрицы ошибок).
- **ucimlrepo**: для загрузки наборов данных из репозитория UCI.
- **numpy**: для эффективных численных вычислений.

## 2. Загрузка и исследование данных

На этом этапе происходит загрузка знаменитого набора данных "Iris" и его первичное исследование.

```python
# Загрузка датасета
iris = fetch_ucirepo(id=53) 

# Извлечение признаков (X) и целевой переменной (y)
X = iris.data.features 
y = iris.data.targets 
dataset = iris.data.original

# Вывод метаданных и самого датасета
print(iris.metadata) 
print(dataset)
```

- `fetch_ucirepo(id=53)` загружает датасет "Iris".
- Данные разделяются на `X` (признаки: длина/ширина чашелистика и лепестка) и `y` (целевая переменная: вид ириса).
- Проводится базовый анализ данных: вывод общей информации, количества объектов по классам, проверка на пропущенные значения, вычисление минимальных, максимальных и средних значений для каждого признака. Это помогает получить общее представление о данных.

## 3. Разделение данных

Данные делятся на обучающую и тестовую выборки. Это стандартная практика в машинном обучении, позволяющая обучить модель на одной части данных, а проверить ее качество на другой, "невидимой" для модели части.

```python
# Разделение данных в пропорции 80% / 20%
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42
)
```
- `test_size=0.2` означает, что 20% данных будет использовано для тестирования, а 80% — для обучения.
- `random_state=42` обеспечивает воспроизводимость результатов: при каждом запуске разделение будет одинаковым.

## 4. Классификация методом Наивного Байеса

### 4.1. Использование 4 признаков

Первая модель, которая строится, — это Гауссовский Наивный Байес.

```python
# Создание и обучение модели
model = GaussianNB()
model.fit(X_train, y_train)

# Предсказания
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Оценка модели
train_cm = confusion_matrix(y_train, y_train_pred)
test_cm = confusion_matrix(y_test, y_test_pred)
```
- Модель обучается на 4 признаках.
- Строятся матрицы ошибок (confusion matrix) для обучающей и тестовой выборок, которые показывают, сколько объектов каждого класса было предсказано правильно, а сколько — неверно.
- Далее определяется функция `calculate_metrics` для расчета ключевых метрик (Точность, Чувствительность, Специфичность) и выводится подробный отчет. На тестовой выборке модель достигает 100% точности.

### 4.2. Использование 2 признаков

Эксперимент повторяется, но на этот раз используются только два признака: `sepal length` и `sepal width`.

```python
X_new = X.drop(["petal length", "petal width"], axis=1)
# ... (повторение обучения и оценки) ...
```
- Точность модели на тестовых данных с двумя признаками падает до 90%. Это говорит о том, что признаки `petal length` и `petal width` были очень важны для точной классификации.

## 5. Классификация методом Дерева Решений

### 5.1. Использование 4 признаков

Строится модель дерева решений.

```python
dt_model_4 = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model_4.fit(X_train, y_train)
```
- `max_depth=3` ограничивает глубину дерева, чтобы избежать переобучения и сделать его более интерпретируемым.
- Модель также достигает 100% точности на тестовых данных.
- **Примечание**: В ноутбуке есть ячейка для визуализации этого дерева, которая вызывает ошибку (`AttributeError: 'list' object has no attribute 'values'`). Это происходит из-за того, что `feature_names` передается как список, а затем к нему применяется `.values`. Правильный код должен выглядеть так:
  ```python
  # Правильный вызов для визуализации
  plot_tree(dt_model_4,
            feature_names=iris.feature_names, # Просто передать список имен
            class_names=target_names,
            filled=True,
            rounded=True)
  ```

### 5.2. Использование 2 признаков

Аналогично, строится дерево решений на двух признаках (`sepal length` и `sepal width`). Точность падает до 73.33%, что еще раз подтверждает важность признаков лепестков. Дерево для этого случая успешно визуализируется.

## 6. Классификация методом Опорных Векторов (SVM)

### 6.1. SVM на 2 признаках

Исследуется метод опорных векторов. Сначала на 2 признаках.

```python
# Выбор 2 признаков и масштабирование
X_2d = X[['sepal length', 'sepal width']].values
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Обучение моделей с разными ядрами
svm_models = {
    'linear': SVC(kernel='linear'),
    'rbf': SVC(kernel='rbf'),
    # ... и другие
}
```
- **Масштабирование**: `StandardScaler` приводит все признаки к одному масштабу (среднее 0, дисперсия 1). Это критически важно для SVM.
- **Ядра**: SVM тестируется с разными "ядрами" (`linear`, `rbf`, `poly`, `sigmoid`), которые определяют, как строится разделяющая граница.
- Лучшую точность (76.67%) показывает ядро `sigmoid`.
- Для каждого ядра строятся красивые визуализации, показывающие разделяющие области.

### 6.2. SVM на 4 признаках и PCA

Теперь SVM применяется ко всем 4 признакам.

```python
# Масштабирование 4 признаков
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Обучение моделей
# ...
```
- С 4 признаками **линейное ядро (`linear`)** достигает **100% точности**.
- **PCA (Метод главных компонент)**: Так как невозможно визуализировать 4D-пространство, используется PCA для "сжатия" 4 признаков в 2 главных компоненты с минимальной потерей информации.
- Модели SVM обучаются на этих 2 компонентах и визуализируются. Точность на PCA-признаках немного ниже (около 90%), но это позволяет наглядно увидеть, как модель разделяет классы.

## 7. Сравнительный анализ и выводы

В последних ячейках ноутбука проводится комплексный сравнительный анализ всех протестированных методов.

- **Сравнение ядер SVM**: Строится график, который показывает, что `linear` ядро является лучшим для данной задачи.
- **Важность признаков**: С помощью `permutation_importance` определяется, какой признак вносит наибольший вклад в предсказание модели. Наиболее важными оказываются `petal width` и `petal length`.
- **Финальная сводка**: Создается итоговая панель с графиками, которая наглядно сравнивает:
    1. Точность всех алгоритмов (SVM и Наивный Байес показывают лучшие результаты).
    2. Производительность разных ядер SVM.
    3. Падение точности при переходе от 4 признаков к 2.
    4. Количество опорных векторов для каждого ядра.

### **Общие выводы из ноутбука:**

1.  **Лучшие модели**: Наивный Байесовский классификатор и SVM с линейным ядром показали наилучший результат (точность 96-100%) при использовании всех 4 признаков.
2.  **Важность признаков**: Признаки, связанные с лепестками (`petal length`, `petal width`), являются наиболее информативными для классификации ирисов. Модели, обученные только на признаках чашелистиков, показывают значительно худшие результаты.
3.  **SVM**: Для данной задачи простое линейное ядро оказалось самым эффективным. Более сложные ядра (rbf, poly) не дали прироста в качестве.
4.  **PCA**: Метод главных компонент является мощным инструментом для снижения размерности и визуализации, но может приводить к небольшой потере точности.
